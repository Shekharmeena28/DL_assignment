{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7dcyg8tiyyv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Can you think of a few applications for a sequence-to-sequence RNN? What about a\n",
        "sequence-to-vector RNN, and a vector-to-sequence RNN?"
      ],
      "metadata": {
        "id": "F6uPyqQPiz2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Recurrent Neural Networks (RNNs) come in various forms, including sequence-to-sequence, sequence-to-vector, and vector-to-sequence models. Here are some common applications for each type:\n",
        "\n",
        "**Sequence-to-Sequence RNN:**\n",
        "1. **Machine Translation:** Translating a sequence of words or sentences from one language to another.\n",
        "2. **Text Summarization:** Generating a concise summary from a long document or article.\n",
        "3. **Speech Recognition:** Converting an audio sequence (speech) into text.\n",
        "4. **Chatbots and Conversational AI:** Generating human-like responses in chat applications.\n",
        "5. **Video Captioning:** Creating textual descriptions or captions for video frames or clips.\n",
        "6. **Time Series Forecasting:** Predicting future values in a time series, such as stock prices or weather data.\n",
        "\n",
        "**Sequence-to-Vector RNN:**\n",
        "1. **Sentiment Analysis:** Analyzing a text sequence (e.g., movie review) and predicting a sentiment score (positive/negative/neutral).\n",
        "2. **Document Classification:** Categorizing a document (e.g., news article) into predefined categories.\n",
        "3. **Named Entity Recognition (NER):** Identifying and classifying named entities (e.g., names, dates, locations) in text.\n",
        "4. **Emotion Detection:** Determining the emotional tone of a text, such as happiness, sadness, or anger.\n",
        "5. **Speech Emotion Recognition:** Predicting the emotional state (e.g., happy, sad, angry) from speech audio.\n",
        "\n",
        "**Vector-to-Sequence RNN:**\n",
        "1. **Image Captioning:** Generating a textual description (sequence) for an input image (vector).\n",
        "2. **Music Generation:** Creating music scores (sequence) from a given set of musical features (vector).\n",
        "3. **Language Modeling:** Predicting the next word in a sentence or text based on the context provided by a vector input.\n",
        "4. **Time Series Anomaly Detection:** Identifying anomalies in time series data given a feature vector.\n",
        "5. **Question Answering:** Generating a textual answer (sequence) to a question posed as a vector.\n",
        "\n",
        "Each type of RNN architecture is suitable for different types of tasks, and they can be adapted and extended to address various machine learning and natural language processing challenges."
      ],
      "metadata": {
        "id": "7PeW7PmBjCQH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. How many dimensions must the inputs of an RNN layer have? What does each dimension\n",
        "represent? What about its outputs?"
      ],
      "metadata": {
        "id": "xLj5xgFFjE9d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The inputs and outputs of an RNN (Recurrent Neural Network) layer typically have specific dimensions, and understanding these dimensions is crucial for working with RNNs. Here's an explanation of the dimensions for both inputs and outputs:\n",
        "\n",
        "**Inputs to an RNN Layer:**\n",
        "1. **Batch Size (Batch Dimension):** This is the number of sequences or samples processed in each batch. It represents how many input sequences are processed simultaneously. In most cases, you set this dimension when defining the input data and training your model.\n",
        "\n",
        "2. **Sequence Length (Time Steps Dimension):** This dimension represents the length of each input sequence. It signifies how many time steps or elements are in each sequence. The length may vary between sequences, but within a batch, all sequences must have the same length.\n",
        "\n",
        "3. **Feature Dimension:** This dimension represents the number of features or input values at each time step. For example, if you are processing text, each time step could correspond to a word, and the feature dimension could represent word embeddings or one-hot encoded vectors.\n",
        "\n",
        "So, the input shape to an RNN layer is typically represented as `(batch_size, sequence_length, feature_dim)`.\n",
        "\n",
        "**Outputs from an RNN Layer:**\n",
        "1. **Batch Size (Batch Dimension):** Similar to the input batch size, it represents the number of sequences or samples in each batch.\n",
        "\n",
        "2. **Sequence Length (Time Steps Dimension):** This dimension is the same as the input sequence length, indicating how many time steps are in the output sequence. For many RNN applications, the output sequence length matches the input sequence length.\n",
        "\n",
        "3. **Hidden State Dimension:** The hidden state dimension represents the internal state of the RNN at each time step. It encapsulates the information learned from the input sequence up to that point. The dimension may vary depending on the RNN architecture (e.g., LSTM, GRU) and the specified number of hidden units or cells.\n",
        "\n",
        "So, the output shape from an RNN layer is typically represented as `(batch_size, sequence_length, hidden_dim)`.\n",
        "\n",
        "It's important to note that in many cases, you may only be interested in the final hidden state or output of the RNN sequence (at the last time step) for downstream tasks like classification or prediction. In such cases, the output shape becomes `(batch_size, hidden_dim)` as the sequence length dimension is reduced to 1.\n",
        "\n",
        "Understanding the dimensions of RNN inputs and outputs is crucial for correctly configuring RNN layers in your neural network architecture."
      ],
      "metadata": {
        "id": "8y0UJ0-TjOJY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. If you want to build a deep sequence-to-sequence RNN, which RNN layers should\n",
        "have return_sequences=True? What about a sequence-to-vector RNN?"
      ],
      "metadata": {
        "id": "47Vwc1YTjRig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When building a deep sequence-to-sequence RNN or a sequence-to-vector RNN, the choice of setting `return_sequences=True` or `return_sequences=False` for RNN layers depends on the specific architecture and goals of the model. Here are some guidelines for both scenarios:\n",
        "\n",
        "**Deep Sequence-to-Sequence RNN:**\n",
        "In a deep sequence-to-sequence RNN, you have multiple RNN layers stacked on top of each other. The layers' configurations depend on whether you want the entire sequence or only the final output.\n",
        "\n",
        "1. **Encoder RNNs:** In the encoder part of the sequence-to-sequence model, you typically set `return_sequences=True` for all RNN layers. This allows the encoder to output sequences at each time step, capturing the input sequence's temporal information.\n",
        "\n",
        "2. **Decoder RNNs:** In the decoder part, it's common to have RNN layers with `return_sequences=True` for intermediate layers to produce sequences. However, the last RNN layer in the decoder should have `return_sequences=False` to generate a single output vector for each time step, which is used for prediction.\n",
        "\n",
        "Here's an example:\n",
        "\n",
        "```python\n",
        "# Encoder\n",
        "encoder_input = Input(shape=(sequence_length, input_dim))\n",
        "encoder_rnn1 = LSTM(hidden_units1, return_sequences=True)(encoder_input)\n",
        "encoder_rnn2 = LSTM(hidden_units2, return_sequences=True)(encoder_rnn1)\n",
        "\n",
        "# Decoder\n",
        "decoder_rnn1 = LSTM(hidden_units3, return_sequences=True)(decoder_input)\n",
        "decoder_rnn2 = LSTM(hidden_units4, return_sequences=False)(decoder_rnn1)\n",
        "```\n",
        "\n",
        "**Sequence-to-Vector RNN:**\n",
        "In a sequence-to-vector RNN, where the goal is to map a sequence to a single output vector (e.g., for classification or sentiment analysis), you typically use RNN layers with `return_sequences=False` throughout the model. This ensures that the final output is a single vector.\n",
        "\n",
        "Here's an example:\n",
        "\n",
        "```python\n",
        "# Sequence-to-Vector RNN\n",
        "input_seq = Input(shape=(sequence_length, input_dim))\n",
        "rnn_layer1 = LSTM(hidden_units1, return_sequences=False)(input_seq)\n",
        "output = Dense(output_dim, activation='softmax')(rnn_layer1)\n",
        "```\n",
        "\n",
        "In summary, for deep sequence-to-sequence RNNs, you often set `return_sequences=True` for all layers in the encoder and intermediate layers in the decoder. For sequence-to-vector RNNs, you typically set `return_sequences=False` for all layers to produce a single output vector. However, the exact configuration may vary depending on your specific use case and architecture."
      ],
      "metadata": {
        "id": "C-SFKM8AjSaW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Suppose you have a daily univariate time series, and you want to forecast the next seven\n",
        "days. Which RNN architecture should you use?"
      ],
      "metadata": {
        "id": "GCmdTOPpjgH0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you have a daily univariate time series and you want to forecast the next seven days, you should use an RNN architecture designed for sequence-to-sequence forecasting. Specifically, you can use a type of RNN known as the \"Encoder-Decoder\" or \"Seq2Seq\" architecture. Here's how you can structure it:\n",
        "\n",
        "**Encoder-Decoder RNN Architecture for Time Series Forecasting:**\n",
        "\n",
        "1. **Encoder:** The encoder processes the historical time series data and extracts relevant features from it. In this case, you would feed the daily data into the encoder, which consists of one or more RNN layers with `return_sequences=True`. This configuration allows the encoder to capture the temporal dependencies in the historical data and produce a sequence of hidden states.\n",
        "\n",
        "2. **Latent Representation:** The final hidden state of the encoder, representing the summary of historical information, is used as the initial state for the decoder.\n",
        "\n",
        "3. **Decoder:** The decoder is responsible for generating forecasts for the next seven days. It also consists of one or more RNN layers, but the critical difference is that you set `return_sequences=True` for the decoder to output a sequence of predictions for each future time step.\n",
        "\n",
        "4. **Output Layer:** The output layer of the decoder can be a Dense layer with a single unit for univariate time series forecasting. You may apply an appropriate activation function (e.g., linear activation for regression tasks) to the output layer.\n",
        "\n",
        "5. **Training Data:** During training, you would have access to historical data as well as the actual values for the next seven days, which serve as target values for the decoder.\n",
        "\n",
        "6. **Loss Function:** You would typically use a loss function like Mean Squared Error (MSE) or Mean Absolute Error (MAE) to measure the discrepancy between the predicted and actual values.\n",
        "\n",
        "Here's a simplified code example using Keras:\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define the input shape (sequence_length, feature_dim)\n",
        "input_seq = Input(shape=(sequence_length, 1))\n",
        "\n",
        "# Encoder\n",
        "encoder_rnn = LSTM(encoder_units, return_sequences=True)(input_seq)\n",
        "\n",
        "# Decoder\n",
        "decoder_rnn = LSTM(decoder_units, return_sequences=True)(encoder_rnn)\n",
        "output = Dense(1)(decoder_rnn)  # Output layer for univariate time series\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs=input_seq, outputs=output)\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Train the model with historical data and target values for the next seven days\n",
        "model.fit(X_train, y_train, epochs=num_epochs, batch_size=batch_size)\n",
        "```\n",
        "\n",
        "In this architecture, the decoder generates forecasts for each of the next seven days. You can adjust the architecture's complexity by varying the number of encoder and decoder units, the number of layers, and other hyperparameters to suit your specific forecasting task."
      ],
      "metadata": {
        "id": "pv5_GQGVjw-Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are the main difficulties when training RNNs? How can you handle them?"
      ],
      "metadata": {
        "id": "RukqOJxjjy0V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Recurrent Neural Networks (RNNs) comes with several challenges, and understanding and addressing these difficulties are crucial for successful training. Here are some of the main difficulties when training RNNs and strategies to handle them:\n",
        "\n",
        "1. **Vanishing and Exploding Gradients:**\n",
        "   - **Issue:** RNNs are prone to vanishing gradients (weights become too small) and exploding gradients (weights become too large) during backpropagation. This affects the network's ability to learn long-range dependencies.\n",
        "   - **Handling:** Use gradient clipping to limit the magnitude of gradients. Additionally, consider using advanced RNN architectures like LSTMs or GRUs that are designed to mitigate vanishing gradient problems.\n",
        "\n",
        "2. **Long-Term Dependencies:**\n",
        "   - **Issue:** RNNs struggle to capture long-term dependencies in sequences, as the information may degrade over time steps.\n",
        "   - **Handling:** Use LSTM (Long Short-Term Memory) or GRU (Gated Recurrent Unit) cells, which are designed to capture and retain information over longer sequences.\n",
        "\n",
        "3. **Overfitting:**\n",
        "   - **Issue:** RNNs can easily overfit to training data, especially when dealing with small datasets.\n",
        "   - **Handling:** Use dropout or recurrent dropout layers within the RNN to regularize the network. Consider early stopping during training to prevent overfitting. You can also increase the amount of training data if possible.\n",
        "\n",
        "4. **Training Time:**\n",
        "   - **Issue:** RNNs can be slow to train, especially when processing long sequences or deep architectures.\n",
        "   - **Handling:** Consider using techniques like batch training to speed up training, or use GPU acceleration to leverage hardware parallelism.\n",
        "\n",
        "5. **Choice of Hyperparameters:**\n",
        "   - **Issue:** Selecting appropriate hyperparameters (e.g., learning rate, batch size, hidden units) can be challenging and may require extensive experimentation.\n",
        "   - **Handling:** Use techniques like grid search or random search to explore hyperparameter combinations. Cross-validation can help assess model performance.\n",
        "\n",
        "6. **Data Preprocessing:**\n",
        "   - **Issue:** Data preprocessing is critical for RNNs, and issues like missing data, outliers, and scaling can affect training.\n",
        "   - **Handling:** Carefully preprocess data, fill missing values, and normalize or standardize input features. Consider using sequence padding to ensure consistent input lengths.\n",
        "\n",
        "7. **Memory Consumption:**\n",
        "   - **Issue:** RNNs can consume significant memory, especially when processing long sequences or using deep architectures.\n",
        "   - **Handling:** Use techniques like mini-batching to limit memory consumption. You can also explore model compression techniques.\n",
        "\n",
        "8. **Choosing the Right Architecture:**\n",
        "   - **Issue:** Selecting the appropriate RNN architecture (e.g., vanilla RNN, LSTM, GRU) for a specific task can be challenging.\n",
        "   - **Handling:** Experiment with different architectures to find the one that best suits your problem. LSTM and GRU networks are often a good starting point due to their ability to handle long sequences.\n",
        "\n",
        "9. **Data Imbalance:**\n",
        "   - **Issue:** Imbalanced sequences in training data can lead to biased models.\n",
        "   - **Handling:** Address class or sequence imbalance by using appropriate sampling techniques or modifying loss functions.\n",
        "\n",
        "10. **Sequential Dependencies:**\n",
        "    - **Issue:** Some tasks require modeling complex sequential dependencies that are not easily captured by traditional RNNs.\n",
        "    - **Handling:** Explore more advanced architectures like attention mechanisms, Transformer networks, or hybrid models to address specific sequence modeling challenges.\n",
        "\n",
        "Addressing these challenges often requires a combination of architectural choices, hyperparameter tuning, and careful data preprocessing to achieve effective training and better RNN performance."
      ],
      "metadata": {
        "id": "MdoE_beEj-ly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Can you sketch the LSTM cell’s architecture?"
      ],
      "metadata": {
        "id": "1mN0o_LVj_nB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! The architecture of an LSTM (Long Short-Term Memory) cell consists of several components that allow it to capture and manage long-term dependencies in sequences. Here's a sketch of the basic architecture of an LSTM cell:\n",
        "\n",
        "![LSTM Cell Architecture](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3b/The_LSTM_cell.png/525px-The_LSTM_cell.png)\n",
        "\n",
        "Key components of the LSTM cell:\n",
        "\n",
        "1. **Input Gate (i):** The input gate controls the flow of information into the cell. It decides which information from the current input and the previous hidden state should be stored in the cell state. It uses the sigmoid activation function to output values between 0 and 1 for each component of the cell state.\n",
        "\n",
        "2. **Forget Gate (f):** The forget gate determines what information in the cell state should be discarded or forgotten. It considers the previous hidden state and the current input, applying the sigmoid activation function to generate forget gate values (between 0 and 1).\n",
        "\n",
        "3. **Cell State (C):** The cell state stores information over time. It can be updated by the input gate and forget gate operations, allowing it to retain important information and forget irrelevant details.\n",
        "\n",
        "4. **Output Gate (o):** The output gate controls what information from the cell state should be used to produce the output or hidden state. It uses the sigmoid activation function to generate values between 0 and 1 for each component of the cell state.\n",
        "\n",
        "5. **Hidden State (h):** The hidden state is the LSTM cell's output. It carries information forward to the next time step or layer in the network. The hidden state is a filtered version of the cell state, influenced by the output gate's operation.\n",
        "\n",
        "The mathematical operations involved in the LSTM cell's architecture include element-wise multiplications, additions, and activations using sigmoid and hyperbolic tangent (tanh) functions. These operations collectively allow the LSTM to regulate the flow of information and handle long-term dependencies while mitigating the vanishing gradient problem associated with traditional RNNs.\n",
        "\n",
        "LSTM cells are often stacked in layers to form deep LSTM networks, enabling them to capture complex sequential patterns in data. Each LSTM cell within a layer operates in a similar manner as described above, passing its hidden state to the next time step or layer."
      ],
      "metadata": {
        "id": "-iBRv-_EkDH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Why would you want to use 1D convolutional layers in an RNN?"
      ],
      "metadata": {
        "id": "YszJ5BcwkJs4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using 1D convolutional layers in conjunction with Recurrent Neural Networks (RNNs) can be beneficial in certain scenarios for several reasons:\n",
        "\n",
        "1. **Local Feature Extraction:** 1D convolutional layers are effective at capturing local patterns and features within sequential data. While RNNs are excellent at capturing long-range dependencies, they might not capture short-term, local patterns as efficiently. Combining convolutional layers with RNNs allows the model to capture both local and global information in the data.\n",
        "\n",
        "2. **Dimensionality Reduction:** Convolutional layers can reduce the dimensionality of the input sequence. This can be particularly helpful when dealing with high-dimensional input data or when you want to reduce computational complexity. Lower-dimensional representations can be passed to the RNN layers, potentially improving training efficiency and reducing memory requirements.\n",
        "\n",
        "3. **Parallel Processing:** Convolutional layers enable parallel processing of the input sequence, which can lead to faster training and inference compared to RNNs, which process sequences sequentially. This is especially valuable when working with long sequences, as the computational cost of RNNs can become prohibitive.\n",
        "\n",
        "4. **Feature Extraction:** Convolutional layers can automatically learn relevant features from the data. By using multiple convolutional filters with different receptive fields, the model can extract various hierarchical features from the input, which can be valuable for subsequent RNN layers to work with.\n",
        "\n",
        "5. **Data Augmentation:** Convolutional layers can be used for data augmentation by applying random variations to the input data, such as noise or small shifts. This can help make the model more robust to variations in the input data.\n",
        "\n",
        "6. **Hybrid Architectures:** Hybrid architectures that combine convolutional layers with RNNs can capture both spatial and temporal dependencies. For tasks that involve both image-like data (e.g., spectrograms) and sequential data (e.g., speech recognition or video analysis), this combination can be highly effective.\n",
        "\n",
        "Examples of applications where 1D convolutional layers and RNNs are combined include speech recognition, natural language processing, time series forecasting, and various tasks involving sequential or time-series data. The choice of whether to use convolutional layers alongside RNNs depends on the specific characteristics of the data and the nature of the task."
      ],
      "metadata": {
        "id": "oi2EDE03kOJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Which neural network architecture could you use to classify videos?"
      ],
      "metadata": {
        "id": "ob8mJ62WkZAx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To classify videos, you can use a variety of neural network architectures that are well-suited for video analysis tasks. The choice of architecture depends on the complexity of the task and the available computational resources. Here are some neural network architectures commonly used for video classification:\n",
        "\n",
        "1. **Convolutional Neural Networks (CNNs):** CNNs are effective for extracting spatial features from individual video frames. To classify videos, you can use a 3D CNN or a two-stream CNN approach:\n",
        "   - **3D CNN:** This architecture extends traditional CNNs to process video data with three dimensions (height, width, and time). It operates directly on video clips, capturing both spatial and temporal features.\n",
        "   - **Two-Stream CNN:** This approach combines two CNN streams—one for spatial (RGB frames) and another for temporal (optical flow or motion information) features. The streams are then fused for classification.\n",
        "\n",
        "2. **Recurrent Neural Networks (RNNs):** RNNs are suitable for modeling temporal dependencies within videos. You can use:\n",
        "   - **LSTM and GRU Networks:** These RNN variants can capture long-term temporal dependencies within video sequences. You can use a 2D CNN to extract spatial features from individual frames and then feed them into an RNN for sequence modeling.\n",
        "   - **ConvLSTM:** Convolutional LSTM combines CNN and LSTM architectures, allowing the model to process video frames with convolutional operations while maintaining sequential memory.\n",
        "\n",
        "3. **3D Convolutional Networks (C3D):** The C3D architecture extends 3D CNNs by including temporal convolutions in addition to spatial convolutions. This enables the network to directly process video clips and capture both spatial and temporal information simultaneously.\n",
        "\n",
        "4. **Two-Stream Networks:** Similar to the two-stream CNN mentioned earlier, you can use separate networks for spatial and temporal information. For spatial information, a 2D CNN processes individual frames, while for temporal information, an RNN or 1D CNN processes optical flow or motion vectors.\n",
        "\n",
        "5. **I3D (Inflated 3D ConvNets):** I3D is a popular architecture that inflates 2D CNNs pre-trained on image data to 3D CNNs for video classification. It leverages pre-trained weights from large image datasets like ImageNet and fine-tunes them for video tasks.\n",
        "\n",
        "6. **Transformer-Based Architectures:** Transformer-based models, originally designed for natural language processing, can also be adapted for video classification. You can use variants like the \"Vision Transformer\" (ViT) or \"Spatiotemporal Transformer\" (STransformer) to process video data.\n",
        "\n",
        "7. **3D Residual Networks (R3D):** Inspired by ResNet, 3D residual networks (R3D) employ residual connections to build deep 3D CNN architectures for video classification.\n",
        "\n",
        "8. **Dense Optical Flow Networks:** These networks focus on modeling optical flow information, which captures motion in videos. Optical flow networks can be combined with other architectures for motion-aware video classification.\n",
        "\n",
        "The choice of architecture depends on factors such as the dataset size, computational resources, and the complexity of the video classification task. In practice, it's common to use pre-trained models (e.g., on ImageNet) and fine-tune them for video classification tasks to benefit from transfer learning. Additionally, ensembling multiple architectures or using attention mechanisms can further improve classification performance for videos."
      ],
      "metadata": {
        "id": "sFem-PgzkhC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Train a classification model for the SketchRNN dataset, available in TensorFlow Datasets."
      ],
      "metadata": {
        "id": "0levUIfzklpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Step 1: Load the SketchRNN dataset\n",
        "dataset_name = \"sketch_rnn/large_train\"\n",
        "dataset, info = tfds.load(name=dataset_name, with_info=True, as_supervised=True)\n",
        "\n",
        "# Step 2: Preprocess the data\n",
        "def preprocess_data(sketch, label):\n",
        "    # Perform preprocessing here (e.g., resizing, normalizing, etc.)\n",
        "    return sketch, label\n",
        "\n",
        "batch_size = 32\n",
        "train_dataset = dataset[\"train\"].map(preprocess_data).shuffle(buffer_size=10000).batch(batch_size)\n",
        "\n",
        "# Step 3: Create the classification model\n",
        "model = tf.keras.Sequential([\n",
        "    # Add layers to your classification model here (e.g., CNN layers, etc.)\n",
        "    # Output layer with the number of classes for classification\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Step 4: Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Step 5: Train the model\n",
        "num_epochs = 10\n",
        "history = model.fit(train_dataset, epochs=num_epochs)\n",
        "\n",
        "# Step 6: Evaluate the model (optional)\n",
        "# You can split the dataset into training and validation sets and evaluate the model's performance.\n",
        "\n",
        "# Step 7: Save the model (optional)\n",
        "# Save the trained model for later use if needed.\n",
        "model.save(\"sketch_rnn_classification_model.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IMP-z3fHktZW",
        "outputId": "40d36fed-c715-4d5f-d63e-05b3e3eba5ad"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "DatasetNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDatasetNotFoundError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a88bc5e78c22>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Step 1: Load the SketchRNN dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"sketch_rnn/large_train\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_supervised\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Step 2: Preprocess the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_datasets/core/logging/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m       \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_datasets/core/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[1;32m    632\u001b[0m       \u001b[0mSplit\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mspecific\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mavailable\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mds_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m   \"\"\"\n\u001b[0;32m--> 634\u001b[0;31m   dbuilder = _fetch_builder(\n\u001b[0m\u001b[1;32m    635\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m       \u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_datasets/core/load.py\u001b[0m in \u001b[0;36m_fetch_builder\u001b[0;34m(name, data_dir, builder_kwargs, try_gcs)\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0mbuilder_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtry_gcs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtry_gcs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mbuilder_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/contextlib.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recreate_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_datasets/core/logging/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m       \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_datasets/core/load.py\u001b[0m in \u001b[0;36mbuilder\u001b[0;34m(name, try_gcs, **builder_kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;31m# If neither the code nor the files are found, raise DatasetNotFoundError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mnot_found_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_datasets/core/load.py\u001b[0m in \u001b[0;36mbuilder\u001b[0;34m(name, try_gcs, **builder_kwargs)\u001b[0m\n\u001b[1;32m    194\u001b[0m   \u001b[0;31m# First check whether we can find the corresponding dataset builder code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m     \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuilder_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mregistered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# Class not found\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/contextlib.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recreate_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_datasets/core/load.py\u001b[0m in \u001b[0;36mbuilder_cls\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    119\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m       \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregistered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimported_builder_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m       \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_builder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetBuilder\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_datasets/core/registered.py\u001b[0m in \u001b[0;36mimported_builder_cls\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_DATASET_REGISTRY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mDatasetNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Dataset {name} not found.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m   \u001b[0mbuilder_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_DATASET_REGISTRY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDatasetNotFoundError\u001b[0m: Dataset sketch_rnn not found.\nAvailable datasets:\n\t- abstract_reasoning\n\t- accentdb\n\t- aeslc\n\t- aflw2k3d\n\t- ag_news_subset\n\t- ai2_arc\n\t- ai2_arc_with_ir\n\t- amazon_us_reviews\n\t- anli\n\t- answer_equivalence\n\t- arc\n\t- asqa\n\t- asset\n\t- assin2\n\t- bair_robot_pushing_small\n\t- bccd\n\t- beans\n\t- bee_dataset\n\t- beir\n\t- big_patent\n\t- bigearthnet\n\t- billsum\n\t- binarized_mnist\n\t- binary_alpha_digits\n\t- ble_wind_field\n\t- blimp\n\t- booksum\n\t- bool_q\n\t- bucc\n\t- c4\n\t- c4_wsrs\n\t- caltech101\n\t- caltech_birds2010\n\t- caltech_birds2011\n\t- cardiotox\n\t- cars196\n\t- cassava\n\t- cats_vs_dogs\n\t- celeb_a\n\t- celeb_a_hq\n\t- cfq\n\t- cherry_blossoms\n\t- chexpert\n\t- cifar10\n\t- cifar100\n\t- cifar100_n\n\t- cifar10_1\n\t- cifar10_corrupted\n\t- cifar10_n\n\t- citrus_leaves\n\t- cityscapes\n\t- civil_comments\n\t- clevr\n\t- clic\n\t- clinc_oos\n\t- cmaterdb\n\t- cnn_dailymail\n\t- coco\n\t- coco_captions\n\t- coil100\n\t- colorectal_histology\n\t- colorectal_histology_large\n\t- common_voice\n\t- conll2002\n\t- conll2003\n\t- controlled_noisy_web_labels\n\t- coqa\n\t- cos_e\n\t- cosmos_qa\n\t- covid19\n\t- covid19sum\n\t- crema_d\n\t- criteo\n\t- cs_restaurants\n\t- curated_breast_imaging_ddsm\n\t- cycle_gan\n\t- d4rl_adroit_door\n\t- d4rl_adroit_hammer\n\t- d4rl_adroit_pen\n\t- d4rl_adroit_relocate\n\t- d4rl_antmaze\n\t- d4rl_mujoco_ant\n\t- d4rl_mujoco_halfcheetah\n\t- d4rl_mujoco_hopper\n\t- d4rl_mujoco_walker2d\n\t- dart\n\t- davis\n\t- deep1b\n\t- deep_weeds\n\t- definite_pronoun_resolution\n\t- dementiabank\n\t- diabetic_retinopathy_detection\n\t- diamonds\n\t- div2k\n\t- dmlab\n\t- doc_nli\n\t- dolphin_number_word\n\t- domainnet\n\t- downsampled_imagenet\n\t- drop\n\t- dsprites\n\t- dtd\n\t- duke_ultrasound\n\t- e2e_cleaned\n\t- efron_morris75\n\t- emnist\n\t- eraser_multi_rc\n\t- esnli\n\t- eurosat\n\t- fashion_mnist\n\t- flic\n\t- flores\n\t- food101\n\t- forest_fires\n\t- fuss\n\t- gap\n\t- geirhos_conflict_stimuli\n\t- gem\n\t- genomics_ood\n\t- german_credit_numeric\n\t- gigaword\n\t- glove100_angular\n\t- glue\n\t- goemotions\n\t- gov_report\n\t- gpt3\n\t- gref\n\t- groove\n\t- grounded_scan\n\t- gsm8k\n\t- gtzan\n\t- gtzan_music_speech\n\t- hellaswag\n\t- higgs\n\t- hillstrom\n\t- horses_or_humans\n\t- howell\n\t- i_naturalist2017\n\t- i_naturalist2018\n\t- i_naturalist2021\n\t- imagenet2012\n\t- imagenet2012_corrupted\n\t- imagenet2012_fewshot\n\t- imagenet2012_multilabel\n\t- imagenet2012_real\n\t- imagenet2012_subset\n\t- imagenet_a\n\t- imagenet_lt\n\t- imagenet_pi\n\t- imagenet_r\n\t- imagenet_resized\n\t- imagenet_sketch\n\t- imagenet_v2\n\t- imagenette\n\t- imagewang\n\t- imdb_reviews\n\t- irc_disentanglement\n\t- iris\n\t- istella\n\t- kddcup99\n\t- kitti\n\t- kmnist\n\t- laion400m\n\t- lambada\n\t- lfw\n\t- librispeech\n\t- librispeech_lm\n\t- libritts\n\t- ljspeech\n\t- lm1b\n\t- locomotion\n\t- lost_and_found\n\t- lsun\n\t- lvis\n\t- malaria\n\t- math_dataset\n\t- math_qa\n\t- mctaco\n\t- media_sum\n\t- mlqa\n\t- mnist\n\t- mnist_corrupted\n\t- movie_lens\n\t- movie_rationales\n\t- movielens\n\t- moving_mnist\n\t- mrqa\n\t- mslr_web\n\t- mt_opt\n\t- mtnt\n\t- multi_news\n\t- multi_nli\n\t- multi_nli_mismatch\n\t- natural_instructions\n\t- natural_questions\n\t- natural_questions_open\n\t- newsroom\n\t- nsynth\n\t- nyu_depth_v2\n\t- ogbg_molpcba\n\t- omniglot\n\t- open_images_challenge2019_detection\n\t- open_images_v4\n\t- openbookqa\n\t- opinion_abstracts\n\t- opinosis\n\t- opus\n\t- oxford_flowers102\n\t- oxford_iiit_pet\n\t- para_crawl\n\t- pass\n\t- patch_camelyon\n\t- paws_wiki\n\t- paws_x_wiki\n\t- penguins\n\t- pet_finder\n\t- pg19\n\t- piqa\n\t- places365_small\n\t- placesfull\n\t- plant_leaves\n\t- plant_village\n\t- plantae_k\n\t- protein_net\n\t- q_re_cc\n\t- qa4mre\n\t- qasc\n\t- quac\n\t- quality\n\t- quickdraw_bitmap\n\t- race\n\t- radon\n\t- reddit\n\t- reddit_disentanglement\n\t- reddit_tifu\n\t- ref_coco\n\t- resisc45\n\t- rlu_atari\n\t- rlu_atari_checkpoints\n\t- rlu_atari_checkpoints_ordered\n\t- rlu_control_suite\n\t- rlu_dmlab_explore_object_rewards_few\n\t- rlu_dmlab_explore_object_rewards_many\n\t- rlu_dmlab_rooms_select_nonmatching_object\n\t- rlu_dmlab_rooms_watermaze\n\t- rlu_dmlab_seekavoid_arena01\n\t- rlu_locomotion\n\t- rlu_rwrl\n\t- robomimic_mg\n\t- robomimic_mh\n\t- robomimic_ph\n\t- robonet\n\t- robosuite_panda_pick_place_can\n\t- rock_paper_scissors\n\t- rock_you\n\t- s3o4d\n\t- salient_span_wikipedia\n\t- samsum\n\t- savee\n\t- scan\n\t- scene_parse150\n\t- schema_guided_dialogue\n\t- sci_tail\n\t- scicite\n\t- scientific_papers\n\t- scrolls\n\t- sentiment140\n\t- shapes3d\n\t- sift1m\n\t- simpte\n\t- siscore\n\t- smallnorb\n\t- smartwatch_gestures\n\t- snli\n\t- so2sat\n\t- speech_commands\n\t- spoken_digit\n\t- squad\n\t- squad_question_generation\n\t- stanford_dogs\n\t- stanford_online_products\n\t- star_cfq\n\t- starcraft_video\n\t- stl10\n\t- story_cloze\n\t- summscreen\n\t- sun397\n\t- super_glue\n\t- svhn_cropped\n\t- symmetric_solids\n\t- tao\n\t- tatoeba\n\t- ted_hrlr_translate\n\t- ted_multi_translate\n\t- tedlium\n\t- tf_flowers\n\t- the300w_lp\n\t- tiny_shakespeare\n\t- titanic\n\t- trec\n\t- trivia_qa\n\t- tydi_qa\n\t- uc_merced\n\t- ucf101\n\t- unified_qa\n\t- universal_dependencies\n\t- unnatural_instructions\n\t- user_libri_audio\n\t- user_libri_text\n\t- vctk\n\t- visual_domain_decathlon\n\t- voc\n\t- voxceleb\n\t- voxforge\n\t- waymo_open_dataset\n\t- web_graph\n\t- web_nlg\n\t- web_questions\n\t- webvid\n\t- wider_face\n\t- wiki40b\n\t- wiki_auto\n\t- wiki_bio\n\t- wiki_dialog\n\t- wiki_table_questions\n\t- wiki_table_text\n\t- wikiann\n\t- wikihow\n\t- wikipedia\n\t- wikipedia_toxicity_subtypes\n\t- wine_quality\n\t- winogrande\n\t- wit\n\t- wit_kaggle\n\t- wmt13_translate\n\t- wmt14_translate\n\t- wmt15_translate\n\t- wmt16_translate\n\t- wmt17_translate\n\t- wmt18_translate\n\t- wmt19_translate\n\t- wmt_t2t_translate\n\t- wmt_translate\n\t- wordnet\n\t- wsc273\n\t- xnli\n\t- xquad\n\t- xsum\n\t- xtreme_pawsx\n\t- xtreme_pos\n\t- xtreme_s\n\t- xtreme_xnli\n\t- yahoo_ltrc\n\t- yelp_polarity_reviews\n\t- yes_no\n\t- youtube_vis\n\nCheck that:\n    - if dataset was added recently, it may only be available\n      in `tfds-nightly`\n    - the dataset name is spelled correctly\n    - dataset class defines all base class abstract methods\n    - the module defining the dataset class is imported\n\nThe builder directory /root/tensorflow_datasets/sketch_rnn/large_train doesn't contain any versions.\nNo builder could be found in the directory: /root/tensorflow_datasets for the builder: sketch_rnn.\nNo registered data_dirs were found in:\n\t- /root/tensorflow_datasets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ye_61dR_kt3k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}