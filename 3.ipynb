{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Is it OK to initialize all the weights to the same value as long as that value is selected\n",
        "randomly using He initialization?"
      ],
      "metadata": {
        "id": "cAu1BNJ-SL_2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing all the weights to the same value, even if that value is selected randomly using He initialization, is generally not recommended in practice. While He initialization helps mitigate the vanishing gradient problem by setting the initial weights with a certain variance, initializing all weights to the same value can still result in suboptimal network behavior for several reasons:\n",
        "\n",
        "1. **Symmetry Breaking:** Initializing all weights to the same value creates symmetry in the network, meaning neurons in the same layer will have identical weights. During training, if neurons start with the same weights, they will learn the same features, defeating the purpose of having multiple neurons in the same layer.\n",
        "\n",
        "2. **Diverse Feature Extraction:** Neural networks benefit from diverse feature extraction capabilities. Randomly initializing weights allows different neurons to extract different features from the input data, making the network more robust and capable of learning complex patterns.\n",
        "\n",
        "3. **Avoiding Local Minima:** Random initialization helps the network start with different initial conditions in terms of weights. This can help the network escape local minima during optimization and explore a broader solution space.\n",
        "\n",
        "4. **Reducing Overfitting:** Weight diversity introduced by random initialization can help reduce overfitting by preventing the network from fitting the noise in the training data.\n",
        "\n",
        "5. **Effective Learning:** Random initialization encourages effective learning by providing a starting point where neurons can begin adapting their weights based on gradients during training.\n",
        "\n",
        "In summary, while He initialization is a good practice for setting the initial weights to an appropriate scale, it's still important to ensure that the weights are randomly initialized to break symmetry and promote diverse feature extraction. Random initialization is a fundamental principle in training effective neural networks, and using the same value for all weights, even if it's randomly selected, is generally discouraged."
      ],
      "metadata": {
        "id": "RqrPtV5bSYIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Is it OK to initialize the bias terms to 0?"
      ],
      "metadata": {
        "id": "f2IRifjySbql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing the bias terms to 0 is a common practice and is generally considered acceptable in many cases. Bias terms serve to shift the activation function's output, allowing the neural network to better model the data. However, initializing bias terms to 0 is a reasonable choice for the following reasons:\n",
        "\n",
        "1. **No Prejudice:** Initializing biases to 0 ensures that the network starts with no initial bias or prejudice towards any specific output. It allows the network to learn and adapt its biases during training based on the data.\n",
        "\n",
        "2. **Simpler Initialization:** Setting bias terms to 0 simplifies the initialization process, making it easier to implement and less prone to errors in practice.\n",
        "\n",
        "3. **Learning Bias:** During training, the neural network learns the appropriate bias values as it adjusts the weights. Over time, the network will adapt the biases to help the model fit the training data.\n",
        "\n",
        "However, there are situations where custom bias initialization may be beneficial. For example:\n",
        "\n",
        "- **Shifted Data:** If the data distribution is significantly shifted away from 0, initializing the biases closer to the expected mean of the data may help the network converge faster.\n",
        "\n",
        "- **Specific Problem Requirements:** In certain cases, domain knowledge or specific problem requirements may suggest non-zero bias initialization values.\n",
        "\n",
        "In deep learning, there are techniques like batch normalization that can adaptively learn and normalize the mean and variance of activations, reducing the reliance on bias terms. Still, initializing bias terms to 0 is a reasonable default practice that often works well, and the network can learn appropriate bias values during training."
      ],
      "metadata": {
        "id": "QK_LK20fSb6w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "q3. Name three advantages of the SELU activation function over ReLU."
      ],
      "metadata": {
        "id": "NqgibxGgScI5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Scaled Exponential Linear Unit (SELU) activation function offers several advantages over the Rectified Linear Unit (ReLU) activation function and other common activation functions. Here are three key advantages of SELU:\n",
        "\n",
        "1. **Self-Normalization:**\n",
        "   - SELU is designed to be a self-normalizing activation function. This means that in deep neural networks with many layers, SELU can automatically maintain stable mean and variance of activations during training.\n",
        "   - As a result, SELU helps mitigate the vanishing and exploding gradient problems, making it easier to train very deep networks without extensive tuning of hyperparameters.\n",
        "\n",
        "2. **Vanishing Gradient Mitigation:**\n",
        "   - SELU's self-normalizing properties help prevent the vanishing gradient problem, which can hinder training deep networks. When gradients are well-behaved, training becomes more stable and faster.\n",
        "   - In contrast, traditional activation functions like ReLU can lead to vanishing gradients, particularly in deep networks, requiring additional techniques like batch normalization or careful weight initialization.\n",
        "\n",
        "3. **Smoothness and Continuity:**\n",
        "   - SELU is a smooth, continuous function that is continuously differentiable over its entire range. This smoothness is beneficial for optimization algorithms like gradient descent, which rely on derivatives.\n",
        "   - The smoothness of SELU helps avoid \"dying ReLU\" units (units that become inactive and never update) and can result in more reliable and consistent convergence during training."
      ],
      "metadata": {
        "id": "uRourG1jScRt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. In which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?"
      ],
      "metadata": {
        "id": "pHMI0z_nScV7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The choice of activation function in a neural network depends on the specific problem, network architecture, and desired properties of the network. Here are some guidelines for when to use different activation functions:\n",
        "\n",
        "1. **SELU (Scaled Exponential Linear Unit):**\n",
        "   - Use SELU when you have a deep neural network with many layers (e.g., deep feedforward networks or recurrent neural networks).\n",
        "   - SELU is beneficial for architectures where automatic normalization of activations is desired, as it can help mitigate vanishing/exploding gradient problems.\n",
        "   - It's particularly useful when you don't want to rely heavily on techniques like batch normalization or extensive hyperparameter tuning.\n",
        "\n",
        "2. **Leaky ReLU (and its variants):**\n",
        "   - Use Leaky ReLU or its variants (e.g., Parametric ReLU, Randomized ReLU) when you want to address the \"dying ReLU\" problem associated with standard ReLU.\n",
        "   - These variants allow a small gradient to flow through when the unit is not active, preventing units from becoming entirely inactive during training.\n",
        "   - Leaky ReLU and its variants can be beneficial when standard ReLU units are leading to dead neurons in your network.\n",
        "\n",
        "3. **ReLU (Rectified Linear Unit):**\n",
        "   - Use ReLU when you have shallow networks or moderately deep networks and your data is not too sensitive to dead neurons.\n",
        "   - ReLU is computationally efficient and has been widely adopted in practice.\n",
        "   - However, be cautious with deep networks, as it may lead to vanishing gradients in very deep architectures.\n",
        "\n",
        "4. **tanh (Hyperbolic Tangent):**\n",
        "   - Use tanh when you want a function that squashes input values to be in the range of -1 to 1.\n",
        "   - It's useful for data that has zero-mean or close-to-zero-mean, as tanh shifts the mean of the activations to be centered around 0.\n",
        "   - tanh is commonly used in recurrent neural networks (RNNs) and some convolutional neural networks (CNNs).\n",
        "\n",
        "5. **Logistic (Sigmoid):**\n",
        "   - Use the logistic sigmoid function when you need binary classification (output in the range [0, 1]).\n",
        "   - Historically, sigmoid was commonly used as an activation function in the hidden layers of shallow networks, but it's less common in deep networks due to vanishing gradients.\n",
        "   - It's still used in output layers for binary classification tasks where the final prediction should be a probability.\n",
        "\n",
        "6. **Softmax:**\n",
        "   - Use the softmax function in the output layer of a neural network when you have a multi-class classification problem.\n",
        "   - Softmax takes an input vector and normalizes it into a probability distribution over multiple classes, making it suitable for multi-class classification problems.\n",
        "   - It ensures that the output probabilities sum to 1, making it easy to interpret the network's output as class probabilities.\n",
        "\n",
        "It's important to note that the choice of activation function may involve experimentation and hyperparameter tuning to find the best fit for a particular problem. Additionally, modern architectures often use combinations of activation functions and regularization techniques to improve model performance."
      ],
      "metadata": {
        "id": "UtN2ayAoScZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?"
      ],
      "metadata": {
        "id": "LztKhTOBSccv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting the momentum hyperparameter too close to 1 (e.g., 0.99999) when using a Stochastic Gradient Descent (SGD) optimizer can lead to several issues and undesirable consequences in the training of neural networks. Momentum is an important hyperparameter that influences the behavior of the optimizer during gradient descent. Here's what may happen:\n",
        "\n",
        "1. **Reduced Learning Rate Effective:**\n",
        "   - When momentum is set close to 1, it effectively increases the learning rate for the components of the gradient that have been consistent in direction over several iterations.\n",
        "   - This means that the optimizer will accelerate in the direction of these consistent gradients, making large steps in parameter space.\n",
        "\n",
        "2. **Overshooting and Oscillations:**\n",
        "   - High momentum can cause the optimizer to overshoot the minimum of the loss function. It can lead to oscillations or divergence in the optimization process.\n",
        "   - The optimizer may bounce back and forth around the minimum rather than converging to it.\n",
        "\n",
        "3. **Slow Convergence Near Minimum:**\n",
        "   - When the optimizer is very close to the minimum of the loss, the high momentum can make it move slowly, causing slow convergence.\n",
        "   - This can result in the network taking a long time to fine-tune the parameters to reach a minimum.\n",
        "\n",
        "4. **Loss Function Instability:**\n",
        "   - High momentum can lead to instability in the loss function. The loss curve may exhibit erratic behavior with sudden spikes and dips.\n",
        "   - This makes it challenging to monitor and diagnose the training process effectively.\n",
        "\n",
        "5. **Difficulty in Escaping Local Minima:**\n",
        "   - In some cases, high momentum can make it difficult for the optimizer to escape local minima. The network may get stuck in suboptimal solutions.\n",
        "\n",
        "To address these issues, it's important to choose an appropriate momentum value that balances the benefits of faster convergence and overcoming local minima with the risk of instability and overshooting. A common range for momentum values is between 0.9 and 0.99, depending on the problem and architecture. Experimentation and validation on a validation dataset are often necessary to find the optimal momentum value for a specific task. It's also common to combine momentum with other optimization techniques like learning rate schedules and adaptive methods (e.g., Adam) for more stable and efficient training."
      ],
      "metadata": {
        "id": "HmdAfXR1TH7d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Name three ways you can produce a sparse model."
      ],
      "metadata": {
        "id": "sDxASfWrTIIe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Producing a sparse model, which means a model with many of its parameters set to zero or close to zero, is valuable for reducing model size, improving efficiency, and increasing interpretability. Here are three ways to produce a sparse model:\n",
        "\n",
        "1. **L1 Regularization (Lasso Regularization):**\n",
        "   - L1 regularization adds a penalty term to the loss function during training that encourages sparsity in the model's weights.\n",
        "   - The penalty term is typically the absolute sum of the weights (L1 norm), and it can be scaled by a hyperparameter called the regularization strength (\\(\\alpha\\)).\n",
        "   - As the model is trained with L1 regularization, some weights tend to become exactly zero, leading to a sparse model.\n",
        "   - Common optimization algorithms for L1 regularization include L1-regularized linear regression, Lasso regression, and L1-regularized neural networks.\n",
        "\n",
        "2. **Pruning Techniques:**\n",
        "   - Pruning involves identifying and removing unimportant weights from a trained model.\n",
        "   - It can be done iteratively during or after training. One common approach is to prune weights with small magnitudes.\n",
        "   - Magnitude-based pruning methods involve setting weights below a certain threshold to zero.\n",
        "   - Structured pruning techniques aim to remove entire neurons, channels, or layers from neural networks while maintaining model performance.\n",
        "   - Pruning can be applied to various types of models, including neural networks, decision trees, and linear models.\n",
        "\n",
        "3. **Feature Selection and Dimensionality Reduction:**\n",
        "   - In some cases, producing a sparse model can involve selecting a subset of relevant features or reducing the dimensionality of the input data.\n",
        "   - Feature selection methods evaluate the importance of each feature and retain only the most informative ones.\n",
        "   - Dimensionality reduction techniques like Principal Component Analysis (PCA) or autoencoders can be used to project high-dimensional data into a lower-dimensional space, effectively reducing the number of features or neurons.\n",
        "   - Sparse models can be trained on the reduced-dimensional data, leading to a sparser representation.\n",
        "\n",
        "Each of these methods has its strengths and weaknesses and may be more suitable for specific tasks or types of models. The choice of which method to use depends on the problem's requirements, the model architecture, and the trade-offs between sparsity and performance."
      ],
      "metadata": {
        "id": "SRl6y-EMTISM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on\n",
        "new instances)? What about MC Dropout?"
      ],
      "metadata": {
        "id": "Q-LDwHqdTW85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dropout** is a regularization technique commonly used in neural networks to prevent overfitting. It works by randomly setting a fraction of neurons (or units) to zero during each training iteration. Dropout can affect the training process and inference differently:\n",
        "\n",
        "1. **Training Time:**\n",
        "   - During training, dropout can slow down the convergence of the neural network to some extent. This is because, at each training iteration, a random subset of neurons is deactivated (set to zero), which effectively reduces the capacity of the network.\n",
        "   - As a result, the network may require more training iterations (epochs) to achieve the desired level of accuracy.\n",
        "   - However, the regularization effect of dropout often compensates for this slowdown by improving the model's ability to generalize to unseen data.\n",
        "\n",
        "2. **Inference Time (Making Predictions on New Instances):**\n",
        "   - During inference (making predictions on new data), dropout is typically turned off. In other words, all neurons are used for inference, and no random dropout is applied.\n",
        "   - As a result, inference with dropout typically does not slow down. In fact, dropout can improve the model's performance on new, unseen data by reducing overfitting, which may lead to faster convergence and better generalization.\n",
        "\n",
        "**MC Dropout (Monte Carlo Dropout):**\n",
        "MC Dropout is an extension of the dropout technique used during inference to obtain uncertainty estimates for model predictions. It involves running inference (making predictions) with dropout enabled multiple times (usually with dropout applied to each run) and aggregating the results. Here's how it affects training and inference:\n",
        "\n",
        "- **Training Time:** MC Dropout has a similar impact on training time as regular dropout because it's primarily a technique used during inference. Training with MC Dropout is not significantly different from training with standard dropout in terms of computational cost.\n",
        "\n",
        "- **Inference Time:** MC Dropout does slow down inference compared to regular inference without dropout because it involves running multiple forward passes through the network with dropout enabled.\n",
        "   - The slowdown depends on the number of dropout samples taken during MC Dropout inference. More samples can provide more accurate uncertainty estimates but also increase computational overhead.\n",
        "   - Despite the slowdown, MC Dropout provides valuable information about the model's uncertainty and can be particularly useful in scenarios where understanding prediction uncertainty is important, such as in medical diagnoses or autonomous driving.\n",
        "\n",
        "In summary, dropout can slow down training to some extent but typically does not slow down inference. MC Dropout introduces a minor inference slowdown due to the need for multiple forward passes, but it provides valuable uncertainty estimates for predictions, which can be beneficial in certain applications. The trade-off between computational cost and the benefits of dropout and MC Dropout should be considered based on the specific requirements of the problem at hand."
      ],
      "metadata": {
        "id": "-iMUT6dMTYAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Practice training a deep neural network on the CIFAR10 image dataset:\n",
        "- a. Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the\n",
        "point of this exercise). Use He initialization and the ELU activation function.\n",
        "- b. Using Nadam optimization and early stopping, train the network on the CIFAR10\n",
        "dataset. You can load it with keras.datasets.cifar10.load_​data(). The dataset is\n",
        "composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for\n",
        "testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons.\n",
        "Remember to search for the right learning rate each time you change the model’s\n",
        "architecture or hyperparameters.\n",
        "- c. Now try adding Batch Normalization and compare the learning curves: Is it\n",
        "converging faster than before? Does it produce a better model? How does it affect\n",
        "training speed?\n",
        "- d. Try replacing Batch Normalization with SELU, and make the necessary adjustements\n",
        "to ensure the network self-normalizes (i.e., standardize the input features, use\n",
        "LeCun normal initialization, make sure the DNN contains only a sequence of dense\n",
        "layers, etc.).\n",
        "- e. Try regularizing the model with alpha dropout. Then, without retraining your model,\n",
        "see if you can achieve better accuracy using MC Dropout."
      ],
      "metadata": {
        "id": "HYqmtkElTdhF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4khvp4UJ_J8",
        "outputId": "6fc2208d-69b5-44ed-b2fb-16449540987b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1563/1563 [==============================] - 36s 15ms/step - loss: 2.0045 - accuracy: 0.2608 - val_loss: 1.8135 - val_accuracy: 0.3335\n",
            "Epoch 2/5\n",
            "1563/1563 [==============================] - 24s 15ms/step - loss: 1.8240 - accuracy: 0.3357 - val_loss: 1.8223 - val_accuracy: 0.3399\n",
            "Epoch 3/5\n",
            "1563/1563 [==============================] - 24s 15ms/step - loss: 1.7541 - accuracy: 0.3663 - val_loss: 1.7651 - val_accuracy: 0.3826\n",
            "Epoch 4/5\n",
            "1563/1563 [==============================] - 26s 17ms/step - loss: 1.7143 - accuracy: 0.3809 - val_loss: 1.6982 - val_accuracy: 0.3998\n",
            "Epoch 5/5\n",
            "1563/1563 [==============================] - 23s 15ms/step - loss: 1.6824 - accuracy: 0.3981 - val_loss: 1.6582 - val_accuracy: 0.3993\n"
          ]
        }
      ],
      "source": [
        "#a\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Load the CIFAR-10 dataset and preprocess it\n",
        "(train_images, train_labels), (test_images, test_labels) = keras.datasets.cifar10.load_data()\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "# Build the deep neural network\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=(32, 32, 3)))  # Input layer\n",
        "for _ in range(20):\n",
        "    model.add(keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'))  # Hidden layers\n",
        "model.add(keras.layers.Dense(10, activation='softmax'))  # Output layer\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='nadam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model with early stopping\n",
        "early_stopping = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
        "history = model.fit(train_images, train_labels, epochs=5,\n",
        "                    validation_data=(test_images, test_labels),\n",
        "                    callbacks=[early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#C\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Load the CIFAR-10 dataset and preprocess it\n",
        "(train_images, train_labels), (test_images, test_labels) = keras.datasets.cifar10.load_data()\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "# Build the deep neural network with Batch Normalization\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=(32, 32, 3)))  # Input layer\n",
        "for _ in range(20):\n",
        "    model.add(keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'))  # Hidden layers\n",
        "    model.add(keras.layers.BatchNormalization())  # Batch Normalization after each Dense layer\n",
        "model.add(keras.layers.Dense(10, activation='softmax'))  # Output layer\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='nadam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model with early stopping\n",
        "early_stopping = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
        "history = model.fit(train_images, train_labels, epochs=5,\n",
        "                    validation_data=(test_images, test_labels),\n",
        "                    callbacks=[early_stopping])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCarsXTzKEjH",
        "outputId": "e00fde07-087d-4293-e93a-262630cf2c02"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1563/1563 [==============================] - 55s 23ms/step - loss: 1.9041 - accuracy: 0.3151 - val_loss: 1.8270 - val_accuracy: 0.3386\n",
            "Epoch 2/5\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 1.7420 - accuracy: 0.3780 - val_loss: 1.8708 - val_accuracy: 0.3392\n",
            "Epoch 3/5\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 1.6779 - accuracy: 0.4016 - val_loss: 1.7643 - val_accuracy: 0.3720\n",
            "Epoch 4/5\n",
            "1563/1563 [==============================] - 34s 21ms/step - loss: 1.6433 - accuracy: 0.4194 - val_loss: 1.8054 - val_accuracy: 0.3660\n",
            "Epoch 5/5\n",
            "1563/1563 [==============================] - 37s 24ms/step - loss: 1.5964 - accuracy: 0.4361 - val_loss: 1.6333 - val_accuracy: 0.4172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#d\n",
        "# Standardize input features\n",
        "mean = train_images.mean(axis=0)\n",
        "std = train_images.std(axis=0)\n",
        "train_images_scaled = (train_images - mean) / std\n",
        "test_images_scaled = (test_images - mean) / std\n",
        "\n",
        "# Modify the model for SELU\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=(32, 32, 3)))  # Input layer\n",
        "for _ in range(20):\n",
        "    model.add(keras.layers.Dense(100, activation='selu', kernel_initializer='lecun_normal'))  # SELU layers\n",
        "model.add(keras.layers.Dense(10, activation='softmax'))  # Output layer\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CsI6jfGdMYF2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#e\n",
        "# MC Dropout during inference\n",
        "def mc_dropout_inference(model, images, n_samples):\n",
        "    predictions = [model(images, training=True) for _ in range(n_samples)]\n",
        "    averaged_predictions = tf.reduce_mean(predictions, axis=0)\n",
        "    return averaged_predictions\n",
        "\n",
        "# Use mc_dropout_inference to make predictions\n",
        "n_samples = 10  # Number of dropout samples\n",
        "predictions = mc_dropout_inference(model, test_images_scaled, n_samples)\n"
      ],
      "metadata": {
        "id": "hKg9VybzP6Ss"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2LKxx25QQVtr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7iOPRs9LQZ8M"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CYBx3YLaQo52"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0c0WxVZCRdle"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}