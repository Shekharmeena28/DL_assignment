{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d14d316b2842414bb9c07b1c2ed55b53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7f5039a53259483a9fe5f1ccbe0d6b23",
              "IPY_MODEL_c733a0544775454985dd85b02be4f6c3",
              "IPY_MODEL_1976c3d79faf45dda7f995faf343deb4"
            ],
            "layout": "IPY_MODEL_5b30696612f1490fbf8557b13179ca01"
          }
        },
        "7f5039a53259483a9fe5f1ccbe0d6b23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b3e0dae08f0428c87095a290c44d0e7",
            "placeholder": "​",
            "style": "IPY_MODEL_e492d106e06946c88bc0750a1aaad6c3",
            "value": "Dl Completed...: 100%"
          }
        },
        "c733a0544775454985dd85b02be4f6c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2de205090df141c88c77edc2f4adb2ea",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c5bea913d98a49b3a8621de0fe2d8363",
            "value": 5
          }
        },
        "1976c3d79faf45dda7f995faf343deb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0cfb924894704e7f8305bf93e4e6097d",
            "placeholder": "​",
            "style": "IPY_MODEL_3c236af031e04778b623a737d8cbcfef",
            "value": " 5/5 [00:04&lt;00:00,  1.01s/ file]"
          }
        },
        "5b30696612f1490fbf8557b13179ca01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b3e0dae08f0428c87095a290c44d0e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e492d106e06946c88bc0750a1aaad6c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2de205090df141c88c77edc2f4adb2ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5bea913d98a49b3a8621de0fe2d8363": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0cfb924894704e7f8305bf93e4e6097d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c236af031e04778b623a737d8cbcfef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What are the advantages of a CNN over a fully connected DNN for image classification?"
      ],
      "metadata": {
        "id": "KWbzV_35Y7O_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolutional Neural Networks (CNNs) offer several advantages over fully connected Deep Neural Networks (DNNs) for image classification tasks:\n",
        "\n",
        "1. **Spatial Hierarchies**: CNNs are designed to capture spatial hierarchies in data. They exploit the local correlations in images, which is crucial for recognizing patterns like edges, textures, and shapes. In contrast, DNNs treat the input as a flattened vector, ignoring spatial relationships.\n",
        "\n",
        "2. **Parameter Efficiency**: CNNs are parameter-efficient. By using shared weights in convolutional layers, they have fewer learnable parameters compared to fully connected DNNs. This reduces the risk of overfitting, especially when training on limited data.\n",
        "\n",
        "3. **Translation Invariance**: CNNs are translation-invariant. Convolutional layers apply the same filters across the entire image, allowing them to detect patterns regardless of their position. DNNs require separate weights for each position, making them less robust to translations.\n",
        "\n",
        "4. **Feature Hierarchies**: CNN architectures typically consist of multiple layers with increasingly abstract features. Lower layers detect simple features like edges, while higher layers combine them to recognize complex patterns. DNNs struggle to capture such hierarchies effectively.\n",
        "\n",
        "5. **Local Connectivity**: CNNs enforce local connectivity. Neurons in a convolutional layer are connected to a small receptive field in the previous layer. This reduces the number of connections and encourages the network to learn localized features.\n",
        "\n",
        "6. **Parameter Sharing**: CNNs use weight sharing. A single set of weights is applied to multiple regions of the input, enabling the network to learn feature detectors that are invariant to location. DNNs lack this inherent weight sharing.\n",
        "\n",
        "7. **Reduced Overfitting**: CNNs are less prone to overfitting due to their smaller parameter space and the use of techniques like max-pooling, which reduces spatial dimensions and prevents the model from learning fine-grained noise.\n",
        "\n",
        "8. **Efficient Computation**: CNNs are computationally efficient, especially when processing large images. The use of shared weights and local connectivity reduces the computational cost compared to fully connected DNNs.\n",
        "\n",
        "9. **Scale Variance**: CNNs can handle inputs of varying sizes through techniques like pooling and resizing. DNNs require fixed-size inputs.\n",
        "\n",
        "10. **Interpretability**: CNNs provide interpretability at different levels of abstraction. You can visualize feature maps to understand what the network is learning, which aids model debugging and analysis.\n",
        "\n",
        "11. **State-of-the-Art Performance**: CNNs have consistently achieved state-of-the-art performance in image classification tasks, including challenges like ImageNet, demonstrating their effectiveness.\n",
        "\n",
        "While CNNs excel in image classification tasks, fully connected DNNs are still valuable for other types of data, such as tabular data or sequences. However, for tasks involving images or spatial data, CNNs are the preferred choice due to their ability to capture spatial hierarchies and patterns efficiently."
      ],
      "metadata": {
        "id": "_WpMvD_hZBUJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Consider a CNN composed of three convolutional layers, each with 3 × 3 kernels, a stride of\n",
        "2, and &quot;same&quot; padding. The lowest layer outputs 100 feature maps, the middle one outputs\n",
        "200, and the top one outputs 400. The input images are RGB images of 200 × 300 pixels.\n",
        "\n",
        "\n",
        "What is the total number of parameters in the CNN? If we are using 32-bit floats, at least how much\n",
        "RAM will this network require when making a prediction for a single instance? What about when\n",
        "training on a mini-batch of 50 images?"
      ],
      "metadata": {
        "id": "54XKkqraZkYN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To calculate the total number of parameters in the CNN and estimate the RAM requirements, we need to consider the following components:\n",
        "\n",
        "1. Convolutional layers: For each convolutional layer, we calculate the number of parameters as follows:\n",
        "   - Number of filters (kernels) in the layer.\n",
        "   - Size of each kernel (3x3).\n",
        "   - Number of input channels (3 for RGB images).\n",
        "   \n",
        "   The number of parameters in a single convolutional layer is calculated as:\n",
        "   \n",
        "   Parameters per layer = (Size of kernel * Number of input channels + 1 (for bias)) * Number of filters\n",
        "\n",
        "2. Pooling layers: Pooling layers typically have no parameters, as they perform downsampling without learnable weights.\n",
        "\n",
        "3. Fully connected layers (if present): We don't have fully connected layers in this CNN architecture.\n",
        "\n",
        "Now, let's calculate the number of parameters for each convolutional layer and then sum them up to find the total number of parameters in the CNN:\n",
        "\n",
        "- Layer 1: 100 filters, 3x3 kernel, 3 input channels\n",
        "  Parameters in Layer 1 = (3 * 3 * 3 + 1) * 100 = 2800 parameters\n",
        "\n",
        "- Layer 2: 200 filters, 3x3 kernel, 100 input channels (output of Layer 1)\n",
        "  Parameters in Layer 2 = (3 * 3 * 100 + 1) * 200 = 180200 parameters\n",
        "\n",
        "- Layer 3: 400 filters, 3x3 kernel, 200 input channels (output of Layer 2)\n",
        "  Parameters in Layer 3 = (3 * 3 * 200 + 1) * 400 = 720400 parameters\n",
        "\n",
        "Total number of parameters in the CNN = Parameters in Layer 1 + Parameters in Layer 2 + Parameters in Layer 3\n",
        "Total parameters = 2800 + 180200 + 720400 = 903400 parameters\n",
        "\n",
        "Now, let's estimate the RAM requirements:\n",
        "\n",
        "RAM per parameter (assuming 32-bit floats) = 32 bits / 8 (bytes per float) = 4 bytes\n",
        "\n",
        "For a single prediction:\n",
        "RAM required for a single prediction = Total parameters * RAM per parameter\n",
        "RAM required for a single prediction = 903400 parameters * 4 bytes/parameter = 3,613,600 bytes ≈ 3.45 MB\n",
        "\n",
        "When training on a mini-batch of 50 images:\n",
        "RAM required for a mini-batch = RAM required for a single prediction * Batch size\n",
        "RAM required for a mini-batch = 3.45 MB/image * 50 images = 172.5 MB\n",
        "\n",
        "So, when making a prediction for a single instance, the network will require approximately 3.45 MB of RAM. When training on a mini-batch of 50 images, the RAM requirement for the mini-batch will be approximately 172.5 MB."
      ],
      "metadata": {
        "id": "EYbNqQMUZneN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. If your GPU runs out of memory while training a CNN, what are five things you could try to\n",
        "solve the problem?"
      ],
      "metadata": {
        "id": "FhUp5sCkZuHf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running out of GPU memory during training is a common issue when training deep neural networks, including CNNs, on large datasets or architectures. Here are five strategies to address this problem:\n",
        "\n",
        "1. **Batch Size Reduction**:\n",
        "   - Decrease the batch size: Smaller batches require less memory. Reduce the batch size to a value that fits within your GPU's memory capacity. However, keep in mind that smaller batches may slow down training due to increased computation overhead.\n",
        "\n",
        "2. **Model Architecture Adjustments**:\n",
        "   - Reduce model complexity: Decrease the number of layers, neurons, or filters in your CNN architecture. This reduces the number of parameters and intermediate activations, saving GPU memory.\n",
        "   - Use smaller kernels: Consider using smaller convolutional kernel sizes (e.g., 3x3 instead of 5x5) to reduce memory usage.\n",
        "   - Remove unnecessary layers: Review your model architecture and remove any unnecessary or redundant layers that do not significantly contribute to performance.\n",
        "\n",
        "3. **Gradient Checkpointing**:\n",
        "   - Implement gradient checkpointing: Instead of storing all intermediate activations during backpropagation, use gradient checkpointing techniques to trade off computation for memory. This reduces GPU memory consumption at the cost of increased computation time.\n",
        "\n",
        "4. **Mixed Precision Training**:\n",
        "   - Use mixed precision training: Some modern GPUs support mixed precision training, which allows you to use lower-precision data types (e.g., float16) for activations and gradients while keeping the model weights in higher precision (e.g., float32). This reduces memory usage without sacrificing training quality.\n",
        "\n",
        "5. **Memory Management**:\n",
        "   - Enable GPU memory growth: In TensorFlow, you can set the GPU memory growth option to allocate GPU memory dynamically, increasing memory utilization efficiency.\n",
        "   - Use gradient accumulation: Instead of updating model weights after each mini-batch, accumulate gradients over multiple mini-batches and perform a weight update less frequently. This can help reduce memory spikes during gradient computation.\n",
        "\n",
        "6. **Data Augmentation**:\n",
        "   - Apply data augmentation: Generate augmented versions of your training data on-the-fly during training. Data augmentation introduces variety without requiring additional memory for storing augmented data samples.\n",
        "\n",
        "7. **Reduce Input Resolution**:\n",
        "   - Decrease input image resolution: If applicable, reduce the input image size. Smaller images require less memory for both input storage and feature map activations.\n",
        "\n",
        "8. **Use Multiple GPUs or Distributed Training**:\n",
        "   - If available, consider using multiple GPUs or distributed training across multiple machines. Distributed training can distribute memory usage and reduce the memory load on each individual GPU.\n",
        "\n",
        "9. **Profiling and Debugging**:\n",
        "   - Use GPU profiling tools to identify memory bottlenecks and memory-hungry operations within your model. Profiling can help pinpoint areas that require optimization.\n",
        "\n",
        "10. **Upgrade Hardware**:\n",
        "    - If possible, consider upgrading to a GPU with more memory capacity to accommodate larger models and batches.\n",
        "\n",
        "It's important to note that the specific strategy you choose depends on the nature of your problem, the available hardware, and your performance requirements. Experiment with these strategies iteratively to find the best trade-offs between memory usage and training efficiency for your particular deep learning task."
      ],
      "metadata": {
        "id": "SSaIS49TZ5EO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Why would you want to add a max pooling layer rather than a convolutional layer with the\n",
        "same stride?"
      ],
      "metadata": {
        "id": "TWD2wFnaZ5-i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding a max pooling layer instead of a convolutional layer with the same stride serves a specific purpose in convolutional neural networks (CNNs) and can offer several advantages:\n",
        "\n",
        "1. **Dimension Reduction**:\n",
        "   - Max pooling reduces the spatial dimensions of feature maps. It down-samples the feature maps, which can help control the growth of computational complexity as we move deeper into the network. This reduction in spatial dimensions is often desirable to limit the number of parameters and computations in the network.\n",
        "\n",
        "2. **Translation Invariance**:\n",
        "   - Max pooling enforces translation invariance to some extent. By selecting the maximum value within a local region, it retains information about the presence of a feature in that region while making the network less sensitive to its precise location. This can improve the model's ability to recognize features in different positions within the receptive field.\n",
        "\n",
        "3. **Reduced Overfitting**:\n",
        "   - Max pooling can introduce a form of regularization by selecting the most important information from each local region. This can help prevent overfitting by reducing the model's reliance on noise or small variations in the data.\n",
        "\n",
        "4. **Computationally Efficient**:\n",
        "   - Max pooling is computationally efficient compared to convolution with the same stride. During max pooling, no learnable parameters are involved, and only the maximum value within each region is computed, reducing the computational cost.\n",
        "\n",
        "5. **Feature Invariance**:\n",
        "   - Max pooling helps in capturing higher-level features that are invariant to small spatial shifts. For example, if a specific edge or texture pattern is detected in one part of an image, max pooling ensures that the same feature is detected regardless of its exact position within the receptive field.\n",
        "\n",
        "6. **Hierarchical Features**:\n",
        "   - Max pooling layers are typically used after convolutional layers to progressively reduce spatial dimensions and focus on higher-level features. This hierarchical representation helps the network capture complex patterns and features.\n",
        "\n",
        "7. **Visualization and Interpretability**:\n",
        "   - Max pooling reduces the spatial resolution, making feature maps more interpretable and easier to visualize. It emphasizes the most important features in each region.\n",
        "\n"
      ],
      "metadata": {
        "id": "mneTQVEIZ9ba"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. When would you want to add a local response normalization layer?"
      ],
      "metadata": {
        "id": "DNmblGSiaHh5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Local Response Normalization (LRN) layer, also known as a normalization layer, was introduced in early convolutional neural network architectures like AlexNet. While it has been used in the past, it is less common in modern CNN architectures. Here are some situations when you might consider adding an LRN layer:\n",
        "\n",
        "1. **Reproduction of Older Architectures**: If you are working on reproducing or studying older CNN architectures like AlexNet, which included LRN layers, you may want to include them to maintain architectural fidelity.\n",
        "\n",
        "2. **Exploration of LRN Effects**: If you are experimenting with different layer types and hyperparameters to understand their effects on model performance, you might add an LRN layer to observe its impact on training and generalization.\n",
        "\n",
        "3. **Specific Architectural Choices**: Some researchers have proposed variations of CNN architectures that incorporate LRN layers for specific reasons. For example, you might find research papers that suggest LRN as a component of a novel architecture designed for a particular task.\n",
        "\n",
        "4. **Handling Local Response Inhibition**: LRN layers were originally introduced to simulate lateral inhibition in the human visual cortex, where neurons that respond strongly to a stimulus inhibit their neighbors' responses. If you believe that a similar mechanism might be beneficial for your task, you could try adding an LRN layer.\n",
        "\n",
        "5. **Reducing Overfitting**: In some cases, LRN layers were used as a form of regularization, although more modern techniques such as dropout and batch normalization are typically preferred for this purpose.\n",
        "\n"
      ],
      "metadata": {
        "id": "vO56F0GRaUos"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Can you name the main innovations in AlexNet, compared to LeNet-5? What about the main\n",
        "innovations in GoogLeNet, ResNet, SENet, and Xception?"
      ],
      "metadata": {
        "id": "_oFa32Lpaeo8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sure, let's briefly summarize the main innovations in each of these influential CNN architectures compared to their predecessors:\n",
        "\n",
        "**AlexNet (2012):**\n",
        "- **Deep Architecture:** AlexNet was significantly deeper compared to previous architectures like LeNet-5, with eight learned layers. It consisted of five convolutional layers followed by three fully connected layers.\n",
        "- **ReLU Activation:** AlexNet used rectified linear units (ReLU) as the activation function, which helped mitigate the vanishing gradient problem and accelerated training.\n",
        "- **Local Response Normalization (LRN):** AlexNet introduced LRN layers to provide local contrast normalization in the network.\n",
        "- **Overlapping Max-Pooling:** AlexNet used max-pooling layers with overlapping regions to reduce spatial dimensions.\n",
        "- **Data Augmentation:** Data augmentation techniques, including random cropping and horizontal flipping, were used to increase the effective size of the training dataset.\n",
        "- **Dropout:** Dropout was employed as a regularization technique to prevent overfitting.\n",
        "- **Large-Scale Training:** AlexNet was trained on a large-scale dataset, ImageNet, which contained millions of images and thousands of categories.\n",
        "\n",
        "**GoogLeNet (Inception, 2014):**\n",
        "- **Inception Modules:** The key innovation in GoogLeNet was the introduction of Inception modules, which consisted of multiple parallel convolutional operations with different kernel sizes and strides. This allowed the network to capture features at multiple scales efficiently.\n",
        "- **Global Average Pooling:** GoogLeNet used global average pooling at the end of the network, replacing the fully connected layers. This reduced the number of parameters and improved spatial invariance.\n",
        "- **Auxiliary Classifiers:** GoogLeNet introduced auxiliary classifiers at intermediate layers during training to combat the vanishing gradient problem.\n",
        "- **Network Depth:** GoogLeNet was deeper than previous networks, with 22 layers in its inception-v1 variant.\n",
        "  \n",
        "**ResNet (2015):**\n",
        "- **Residual Blocks:** ResNet introduced residual blocks, where the output of a convolutional layer was added to the input through a skip connection (shortcut connection). This allowed for training very deep networks (hundreds of layers) without suffering from vanishing gradients.\n",
        "- **Deep Architectures:** ResNet demonstrated the benefits of extremely deep networks and achieved state-of-the-art performance with its deeper variants (e.g., ResNet-152).\n",
        "- **Batch Normalization:** Batch normalization was employed to stabilize and accelerate training.\n",
        "\n",
        "**SENet (Squeeze-and-Excitation Network, 2017):**\n",
        "- **SE Blocks:** SENet introduced SE blocks, which learned channel-wise feature recalibration. These blocks dynamically weighted feature maps to emphasize important channels and suppress less relevant ones.\n",
        "- **Attention Mechanism:** SENet incorporated a self-attention mechanism to improve the network's ability to focus on informative features.\n",
        "\n",
        "**Xception (2017):**\n",
        "- **Separable Convolutions:** Xception introduced depthwise separable convolutions, which separate the spatial and channel-wise convolutions. This reduced the number of parameters and computational cost while maintaining expressive power.\n",
        "- **Extreme Depth:** Xception demonstrated that very deep networks with efficient building blocks could achieve state-of-the-art performance.\n",
        "\n",
        "These architectures represent significant milestones in the evolution of deep learning and have inspired many subsequent architectures. Each introduced novel concepts and techniques to improve the training and performance of convolutional neural networks."
      ],
      "metadata": {
        "id": "-CMDrNXIafxA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What is a fully convolutional network? How can you convert a dense layer into a\n",
        "convolutional layer?"
      ],
      "metadata": {
        "id": "R0N-hns0atKD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Fully Convolutional Network (FCN) is a type of neural network architecture designed for tasks involving structured grid data, such as image segmentation, where the input and output are both of spatial dimensions. FCNs are characterized by the absence of fully connected layers and their heavy reliance on convolutional layers.\n",
        "\n",
        "To convert a dense (fully connected) layer into a convolutional layer, you need to perform the following steps:\n",
        "\n",
        "1. **Transpose the Weights:**\n",
        "   - Take the weight matrix of the dense layer and transpose it. The resulting weight tensor will have dimensions (output_channels, input_channels).\n",
        "\n",
        "2. **Create a New Convolutional Layer:**\n",
        "   - Create a new convolutional layer with the same number of output channels as the original dense layer's number of neurons.\n",
        "   - The kernel size of the convolutional layer should be (1, 1), meaning it operates on a single spatial location and does not perform spatial filtering.\n",
        "\n",
        "3. **Initialize Weights:**\n",
        "   - Initialize the weights of the new convolutional layer with the transposed weight tensor from step 1. These weights capture the same linear transformations as the original dense layer.\n",
        "\n",
        "4. **Use Appropriate Activation Function:**\n",
        "   - If the original dense layer had an activation function (e.g., ReLU), apply the same activation function to the output of the new convolutional layer.\n",
        "\n",
        "5. **Connect Layers:**\n",
        "   - Connect the new convolutional layer to the previous layers in the network, ensuring that the tensor shapes match. Typically, this involves adjusting the shape of the input to the new convolutional layer to match the spatial dimensions of the previous layer's output.\n",
        "\n"
      ],
      "metadata": {
        "id": "_LqLxDmnayRH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the main technical difficulty of semantic segmentation?"
      ],
      "metadata": {
        "id": "MrRe97Lca7Zn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main technical difficulty of semantic segmentation is the high level of detail and fine-grained understanding required to accurately classify and segment objects and regions within an image. Semantic segmentation aims to assign a class label to every pixel in an image, indicating the object or category to which it belongs. This task presents several significant challenges:\n",
        "\n",
        "1. **Pixel-Level Accuracy:** Unlike object detection, where bounding boxes are used to approximate object locations, semantic segmentation requires pixel-level accuracy. Each pixel in the image must be correctly classified, and the boundaries between different objects or regions must be delineated accurately.\n",
        "\n",
        "2. **High-Resolution Inputs:** Semantic segmentation often works with high-resolution images, which means there are a large number of pixels to classify. This increases the computational complexity and memory requirements of the task.\n",
        "\n",
        "3. **Object Occlusion and Interactions:** In real-world scenes, objects can be partially or completely occluded by other objects, and they can interact in complex ways. Distinguishing objects in such scenarios is challenging.\n",
        "\n",
        "4. **Class Imbalance:** Some classes may be significantly more common than others in the dataset, leading to class imbalance issues. Models must be able to handle this imbalance to avoid bias toward the majority classes.\n",
        "\n",
        "5. **Spatial Consistency:** Maintaining spatial consistency in segmentations is crucial. For example, if a network segments a car in one part of the image, it should also correctly segment the car in other parts of the image, even if the car's appearance varies due to changes in lighting, pose, or scale.\n",
        "\n",
        "6. **Fine-Grained Object Recognition:** Distinguishing between fine-grained object categories or subtle differences within object categories (e.g., different breeds of dogs) requires a high level of detail in segmentation.\n",
        "\n",
        "7. **Generalization:** Models must generalize well to handle various environmental conditions, object orientations, and backgrounds that were not necessarily present in the training data.\n",
        "\n",
        "8. **Edge and Boundary Handling:** Precise object boundaries are crucial for accurate segmentation. Models need to handle edges and boundaries effectively, as errors in these areas can significantly impact the quality of the segmentation.\n",
        "\n",
        "9. **Real-Time Requirements:** In some applications, such as autonomous driving, semantic segmentation needs to be performed in real-time, imposing constraints on computational efficiency.\n",
        "\n",
        "To address these challenges, researchers in computer vision and deep learning have developed advanced techniques and architectures, including Fully Convolutional Networks (FCNs), U-Net, DeepLab, and others. These models incorporate techniques like skip connections, dilated convolutions, and multi-scale processing to improve segmentation accuracy and address the technical difficulties associated with semantic segmentation. Additionally, large-scale annotated datasets and data augmentation strategies have played a crucial role in training effective segmentation models."
      ],
      "metadata": {
        "id": "EdjXKUM3bEJk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Build your own CNN from scratch and try to achieve the highest possible accuracy on MNIST."
      ],
      "metadata": {
        "id": "UDrlBLL7bKv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "FpiJfJT5Y-kv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEzW734iclYV",
        "outputId": "5e763170-96af-470b-a56d-0825b1b4317c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Load and preprocess the MNIST dataset\n",
        "mnist = keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0  # Normalize pixel values to [0, 1]\n",
        "\n",
        "# Build the CNN model\n",
        "model = keras.Sequential([\n",
        "    layers.Reshape((28, 28, 1), input_shape=(28, 28)),  # Reshape input for convolution\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),  # Dropout for regularization\n",
        "    layers.Dense(10, activation='softmax')  # Output layer with 10 classes (digits 0-9)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_images, train_labels, epochs=10, batch_size=64, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
        "print(f'Test accuracy: {test_accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuRosPhvc_xX",
        "outputId": "3fa14987-30bf-46ab-a88b-9755b3ace63a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "844/844 [==============================] - 30s 34ms/step - loss: 0.2647 - accuracy: 0.9198 - val_loss: 0.0531 - val_accuracy: 0.9845\n",
            "Epoch 2/10\n",
            "844/844 [==============================] - 27s 32ms/step - loss: 0.0914 - accuracy: 0.9739 - val_loss: 0.0449 - val_accuracy: 0.9860\n",
            "Epoch 3/10\n",
            "844/844 [==============================] - 27s 32ms/step - loss: 0.0696 - accuracy: 0.9793 - val_loss: 0.0356 - val_accuracy: 0.9898\n",
            "Epoch 4/10\n",
            "844/844 [==============================] - 26s 31ms/step - loss: 0.0547 - accuracy: 0.9839 - val_loss: 0.0316 - val_accuracy: 0.9912\n",
            "Epoch 5/10\n",
            "844/844 [==============================] - 27s 32ms/step - loss: 0.0472 - accuracy: 0.9858 - val_loss: 0.0309 - val_accuracy: 0.9917\n",
            "Epoch 6/10\n",
            "844/844 [==============================] - 27s 32ms/step - loss: 0.0402 - accuracy: 0.9879 - val_loss: 0.0326 - val_accuracy: 0.9918\n",
            "Epoch 7/10\n",
            "844/844 [==============================] - 26s 31ms/step - loss: 0.0353 - accuracy: 0.9892 - val_loss: 0.0310 - val_accuracy: 0.9922\n",
            "Epoch 8/10\n",
            "844/844 [==============================] - 26s 31ms/step - loss: 0.0305 - accuracy: 0.9905 - val_loss: 0.0324 - val_accuracy: 0.9920\n",
            "Epoch 9/10\n",
            "844/844 [==============================] - 26s 31ms/step - loss: 0.0278 - accuracy: 0.9916 - val_loss: 0.0295 - val_accuracy: 0.9920\n",
            "Epoch 10/10\n",
            "844/844 [==============================] - 26s 31ms/step - loss: 0.0252 - accuracy: 0.9917 - val_loss: 0.0303 - val_accuracy: 0.9928\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.0249 - accuracy: 0.9917\n",
            "Test accuracy: 99.17%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Use transfer learning for large image classification, going through these steps:\n",
        "- a. Create a training set containing at least 100 images per class. For example, you could\n",
        "classify your own pictures based on the location (beach, mountain, city, etc.), or\n",
        "alternatively you can use an existing dataset (e.g., from TensorFlow Datasets).\n",
        "- b. Split it into a training set, a validation set, and a test set.\n",
        "- c. Build the input pipeline, including the appropriate preprocessing operations, and\n",
        "optionally add data augmentation.\n",
        "- d. Fine-tune a pretrained model on this dataset."
      ],
      "metadata": {
        "id": "wqupOWJAgM57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Step 1: Load and Split the Dataset\n",
        "# Use TensorFlow Datasets to load a dataset (e.g., 'tf_flowers') and split it into train, validation, and test sets.\n",
        "(train_dataset, validation_dataset, test_dataset), dataset_info = tfds.load(\n",
        "    'tf_flowers',\n",
        "    split=['train[:70%]', 'train[70%:85%]', 'train[85%:]'],\n",
        "    with_info=True,\n",
        "    as_supervised=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "d14d316b2842414bb9c07b1c2ed55b53",
            "7f5039a53259483a9fe5f1ccbe0d6b23",
            "c733a0544775454985dd85b02be4f6c3",
            "1976c3d79faf45dda7f995faf343deb4",
            "5b30696612f1490fbf8557b13179ca01",
            "7b3e0dae08f0428c87095a290c44d0e7",
            "e492d106e06946c88bc0750a1aaad6c3",
            "2de205090df141c88c77edc2f4adb2ea",
            "c5bea913d98a49b3a8621de0fe2d8363",
            "0cfb924894704e7f8305bf93e4e6097d",
            "3c236af031e04778b623a737d8cbcfef"
          ]
        },
        "id": "eE9tDGUdeWNQ",
        "outputId": "35923185-715b-4b75-eaaf-625308d158ab"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset 218.21 MiB (download: 218.21 MiB, generated: 221.83 MiB, total: 440.05 MiB) to /root/tensorflow_datasets/tf_flowers/3.0.1...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dl Completed...:   0%|          | 0/5 [00:00<?, ? file/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d14d316b2842414bb9c07b1c2ed55b53"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset tf_flowers downloaded and prepared to /root/tensorflow_datasets/tf_flowers/3.0.1. Subsequent calls will reuse this data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Preprocess the Data\n",
        "# Define preprocessing functions for resizing, normalizing, and augmenting images.\n",
        "def preprocess_image(image, label):\n",
        "    image = tf.image.resize(image, (224, 224))  # Resize to match pretrained model input size.\n",
        "    image = tf.keras.applications.mobilenet.preprocess_input(image)  # Normalize pixel values.\n",
        "    return image, label"
      ],
      "metadata": {
        "id": "ZZda-Q65etfH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing to the datasets.\n",
        "train_dataset = train_dataset.map(preprocess_image)\n",
        "validation_dataset = validation_dataset.map(preprocess_image)\n",
        "test_dataset = test_dataset.map(preprocess_image)\n"
      ],
      "metadata": {
        "id": "YFxiUd4De9lN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch and shuffle the datasets.\n",
        "batch_size = 32\n",
        "train_dataset = train_dataset.shuffle(1000).batch(batch_size)\n",
        "validation_dataset = validation_dataset.batch(batch_size)\n",
        "test_dataset = test_dataset.batch(batch_size)"
      ],
      "metadata": {
        "id": "Fz7hl5Z5g-Cp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = tf.keras.applications.MobileNetV2(\n",
        "    input_shape=(224, 224, 3),\n",
        "    include_top=False,  # Exclude the top (classification) layer.\n",
        "    weights='imagenet'  # Use weights pre-trained on ImageNet.\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-LxmyIuhDHV",
        "outputId": "dc7501f1-edcf-4520-b4af-1a7b624880b1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9406464/9406464 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = dataset_info.features['label'].num_classes"
      ],
      "metadata": {
        "id": "fe82rX_ThHjr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91mqmMkohPix",
        "outputId": "9368cff2-4c77-4df2-d322-ce4ef22ee5f8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    base_model,\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),  # Reduce spatial dimensions.\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')  # Output layer for classification.\n",
        "])\n"
      ],
      "metadata": {
        "id": "H_iYLEOlhQ6p"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model with an appropriate optimizer and loss function.\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0001),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qutNPPQhZg8",
        "outputId": "94cdc6cf-287f-4e3f-d344-4a42aebd8662"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "history = model.fit(train_dataset,\n",
        "                    validation_data=validation_dataset,\n",
        "                    epochs=epochs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "eFTTtJ8_hTCc",
        "outputId": "9b1113b4-f8f8-4eb8-b878-88b4f55325c9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "28/81 [=========>....................] - ETA: 3:19 - loss: 0.4857 - accuracy: 0.8393"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-e8be4352f15e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history = model.fit(train_dataset,\n\u001b[0m\u001b[1;32m      3\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     epochs=epochs)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1740\u001b[0m                         ):\n\u001b[1;32m   1741\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1742\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1743\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1744\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    855\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 857\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    858\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m       (concrete_function,\n\u001b[1;32m    147\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    149\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1347\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1348\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_call_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1350\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1457\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1458\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
        "print(f'Test accuracy: {test_accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "id": "r-iTGpu4hVuJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}